{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8cf3b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, torchaudio, librosa, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ba32d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(),\n",
    "        )\n",
    "        self.fc_enc = nn.Linear(256*8*14, latent_dim)\n",
    "        self.fc_dec = nn.Linear(latent_dim, 256*8*14)\n",
    "\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 4, 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        z = self.enc(x)\n",
    "        z_flat = z.view(B, -1)\n",
    "        latent = self.fc_enc(z_flat)\n",
    "\n",
    "        out = self.fc_dec(latent)\n",
    "        out = out.view(B, 256, 8, 14)\n",
    "        out = self.dec(out)\n",
    "\n",
    "        out = F.interpolate(out, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
    "        return out, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3a7ec1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ConvAutoencoder(latent_dim=128).to(device)\n",
    "\n",
    "state_dict = torch.load(\"checkpoints/conv_autoencoder.pth\", map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f451fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 22050\n",
    "N_FFT = 1024\n",
    "HOP = 512\n",
    "N_MELS = 128\n",
    "SEG_DUR = 5.0\n",
    "SEG_SAMPLES = int(SR * SEG_DUR)\n",
    "\n",
    "mel = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SR,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP,\n",
    "    n_mels=N_MELS\n",
    ")\n",
    "\n",
    "def preprocess(path, duration=SEG_DUR):\n",
    "    wav, sr = torchaudio.load(path)\n",
    "    if wav.shape[0] > 1:\n",
    "        wav = wav.mean(dim=0, keepdim=True)  # mono\n",
    "    if sr != SR:\n",
    "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
    "    wav = wav.squeeze(0)\n",
    "\n",
    "    # pad/trim\n",
    "    target_len = int(SR * duration)\n",
    "    if len(wav) < target_len:\n",
    "        wav = torch.nn.functional.pad(wav, (0, target_len - len(wav)))\n",
    "    else:\n",
    "        wav = wav[:target_len]\n",
    "\n",
    "    mel_spec = mel(wav.unsqueeze(0))\n",
    "    logmel = torch.log(mel_spec + 1e-6)\n",
    "    return logmel.unsqueeze(0)  # [1, 1, n_mels, time]\n",
    "\n",
    "def beat_align(pathA, pathB):\n",
    "    yA, srA = librosa.load(pathA, sr=SR)\n",
    "    yB, srB = librosa.load(pathB, sr=SR)\n",
    "\n",
    "    tempoA, _ = librosa.beat.beat_track(y=yA, sr=srA)\n",
    "    tempoB, _ = librosa.beat.beat_track(y=yB, sr=srB)\n",
    "\n",
    "    tempoA = float(tempoA)\n",
    "    tempoB = float(tempoB)\n",
    "\n",
    "    print(f\"Tempo A: {tempoA:.2f} BPM, Tempo B: {tempoB:.2f} BPM\")\n",
    "\n",
    "    if tempoB > 0:\n",
    "        rate = tempoA / tempoB\n",
    "        # STFT of song B\n",
    "        D = librosa.stft(yB)\n",
    "        # Time-stretch in STFT domain\n",
    "        D_stretch = librosa.phase_vocoder(D, rate=rate, hop_length=HOP)\n",
    "        # Invert back to waveform\n",
    "        yB_aligned = librosa.istft(D_stretch, hop_length=HOP)\n",
    "    else:\n",
    "        print(\"⚠️ Could not estimate tempo for song B, skipping time-stretch\")\n",
    "        yB_aligned = yB\n",
    "\n",
    "    return yA, yB_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4a5fb301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_latents(fileA, fileB, steps=10):\n",
    "    xA = preprocess(fileA).to(device)\n",
    "    xB = preprocess(fileB).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, zA = model(xA)\n",
    "        _, zB = model(xB)\n",
    "\n",
    "    latents = []\n",
    "    for alpha in np.linspace(0, 1, steps):\n",
    "        z_mix = (1 - alpha) * zA + alpha * zB\n",
    "        latents.append(z_mix)\n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "72aacb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_latents(latents):\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for z in latents:\n",
    "            out = model.fc_dec(z)\n",
    "            out = out.view(-1, 256, 8, 14)\n",
    "            out = model.dec(out)\n",
    "            out = F.interpolate(out, size=(N_MELS, int(SEG_SAMPLES/HOP)), \n",
    "                                mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "            spec = out.squeeze(0).cpu().numpy()\n",
    "            spec = np.exp(spec) - 1e-6  # invert log\n",
    "            wav = librosa.feature.inverse.mel_to_audio(\n",
    "                spec, sr=SR, hop_length=HOP, n_fft=N_FFT\n",
    "            )\n",
    "            outputs.append(wav)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dd55c02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/48/jjk7q1v14vj5tssyzvrpzs5r0000gn/T/ipykernel_57750/9749218.py:41: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  tempoA = float(tempoA)\n",
      "/var/folders/48/jjk7q1v14vj5tssyzvrpzs5r0000gn/T/ipykernel_57750/9749218.py:42: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  tempoB = float(tempoB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo A: 117.45 BPM, Tempo B: 117.45 BPM\n",
      "Shape: (876544,) dtype: float32\n",
      "✅ Transition saved as transition.wav\n"
     ]
    }
   ],
   "source": [
    "fileA = \"input/billie_jean.mp3\"\n",
    "fileB = \"input/get_lucky.mp3\"\n",
    "\n",
    "dir_song_aligned = \"output/songB_aligned.wav\"\n",
    "\n",
    "# Align BPMs\n",
    "yA, yB = beat_align(fileA, fileB)\n",
    "\n",
    "# Save aligned version of songB for consistency\n",
    "sf.write(dir_song_aligned, yB, SR)\n",
    "\n",
    "# Interpolate in latent space\n",
    "latents = interpolate_latents(fileA, dir_song_aligned, steps=8)\n",
    "\n",
    "# Decode to audio\n",
    "outputs = decode_latents(latents)\n",
    "\n",
    "# Concatenate into one transition\n",
    "transition = np.concatenate([o.astype(np.float32).flatten() for o in outputs])\n",
    "\n",
    "print(\"Shape:\", transition.shape, \"dtype:\", transition.dtype)\n",
    "\n",
    "sf.write(\"output/transition.wav\", transition, SR)\n",
    "print(\"✅ Transition saved as transition.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-informatics (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
